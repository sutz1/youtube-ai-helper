import os
import sys
from pathlib import Path
from faster_whisper import WhisperModel, BatchedInferencePipeline
from prompts import previous_description
import ollama

# ----------------- CUDA PATH SETUP (for Windows + GPU use) -----------------
def set_cuda_paths():
    venv_base = Path(sys.executable).parent.parent
    nvidia_base_path = venv_base / 'Lib' / 'site-packages' / 'nvidia'
    cuda_path = nvidia_base_path / 'cuda_runtime' / 'bin'
    cublas_path = nvidia_base_path / 'cublas' / 'bin'
    cudnn_path = nvidia_base_path / 'cudnn' / 'bin'
    paths_to_add = [str(cuda_path), str(cublas_path), str(cudnn_path)]
    env_vars = ['CUDA_PATH', 'CUDA_PATH_V12_4', 'PATH']
    
    for env_var in env_vars:
        current_value = os.environ.get(env_var, '')
        new_value = os.pathsep.join(paths_to_add + [current_value] if current_value else paths_to_add)
        os.environ[env_var] = new_value

# ----------------- TRANSCRIPTION -----------------
def transcribe_audio(audio_path, output_path="transcription_output.txt", model_size="large-v3"):
    set_cuda_paths()
    try:
        model = WhisperModel(model_size, device="cuda", compute_type="float16")
        batched_model = BatchedInferencePipeline(model=model)
        segments, info = batched_model.transcribe(audio_path, batch_size=16)

        with open(output_path, "w", encoding="utf-8") as f:
            f.write("Detected language: {}\n".format(info.language))
            f.write("Language probability: {}\n\n".format(info.language_probability))
            for segment in segments:
                f.write("[%.2fs -> %.2fs] %s\n" % (segment.start, segment.end, segment.text))
        
        print(f"âœ… Transcription completed. Output saved to {output_path}")
    except Exception as e:
        print(f"âŒ Error during transcription: {e}")

# ----------------- CHUNKING -----------------
def split_text_into_chunks(text, max_words=3000):
    words = text.split()
    return [" ".join(words[i:i+max_words]) for i in range(0, len(words), max_words)]

# ----------------- OLLAMA GENERATION -----------------
def generate_from_chunks(full_transcript_text, model="llama3.2"):
    chunks = split_text_into_chunks(full_transcript_text)
    all_outputs = []

    for i, chunk in enumerate(chunks):
        print(f"ğŸ” Processing chunk {i+1}/{len(chunks)}...")

        prompt = f"""
You are a YouTube assistant. A new episode of a podcast about the video game The Finals has been transcribed.
Use the text below to create the following:
- A suitable, clickable **title**
- A 4â€“6 sentence **description** for YouTube
- 3â€“6 **timestamps** with short labels

âš ï¸ Do NOT reuse anything from this example â€” it is only for tone reference:
"{previous_description}"

--- Begin Transcript Chunk ---
{chunk}
--- End Transcript Chunk ---
"""

        response = ollama.chat(
            model=model,
            messages=[{'role': 'user', 'content': prompt}],
            stream=False,
        )

        content = response['message']['content']
        all_outputs.append((i + 1, content))
    
    return all_outputs

# ----------------- OPTIONAL FINAL SUMMARY -----------------
def merge_chunk_summaries(chunks_outputs, model="llama3.2"):
    combined = "\n\n".join([f"Chunk {i}:\n{output}" for i, output in chunks_outputs])

    final_prompt = f"""
Based on the following chunked summaries of a podcast, please combine them into a single:
1. YouTube **Title**
2. 4â€“6 sentence **Description**
3. Timestamp list (3â€“6 items)

Here are the chunks:
{combined}
"""

    response = ollama.chat(
        model=model,
        messages=[{'role': 'user', 'content': final_prompt}],
        stream=False,
    )

    return response['message']['content']

# ----------------- MAIN PIPELINE -----------------
def run_pipeline(audio_path):
    print("ğŸ—£ï¸ Starting transcription...")
    transcribe_audio(audio_path)
    
    print(f"ğŸ“„ Transcription complete. Reading from transcription_out.txt")
    # Step 2: Read and chunk
    with open("transcription_output.txt", "r", encoding="utf-8") as f:
        transcript_text = f.read()
        print("ğŸ“œ Transcript loaded.")

    print("ğŸ” Splitting transcript into chunks...")
    chunk_outputs = generate_from_chunks(transcript_text)

    print("ğŸ“ Saving chunk outputs...")
    with open("chunked_outputs.txt", "w", encoding="utf-8") as f:
        for i, output in chunk_outputs:
            f.write(f"\n--- CHUNK {i} ---\n\n{output}\n")

    print("ğŸ§  Generating final summary...")
    final_summary = merge_chunk_summaries(chunk_outputs)

    with open("final_youtube_summary.txt", "w", encoding="utf-8") as f:
        f.write(final_summary)
    
    print("âœ… All steps complete. Outputs saved to 'chunked_outputs.txt' and 'final_youtube_summary.txt'")

# ----------------- ENTRYPOINT -----------------
if __name__ == "__main__":
    AUDIO_FILE_PATH = r"src\STRIKE A POSE IS BACK! _ The Finals Update 6.5 Overview.mp3"
    run_pipeline(AUDIO_FILE_PATH)